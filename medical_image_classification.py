# -*- coding: utf-8 -*-
"""Medical Image Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# medical_image_classifier.py
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.applications import VGG16, ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from PIL import Image
import cv2

class MedicalImageClassifier:
    def __init__(self, img_size=(224, 224), batch_size=32):
        self.img_size = img_size
        self.batch_size = batch_size
        self.model = None
        self.history = None
        self.class_names = ['NORMAL', 'PNEUMONIA']

    def create_data_generators(self, train_dir, val_dir, test_dir):
        """Create data generators with augmentation"""
        # Training data augmentation
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True,
            zoom_range=0.2,
            shear_range=0.1,
            fill_mode='nearest'
        )

        # Validation/Test data normalization only
        val_test_datagen = ImageDataGenerator(rescale=1./255)

        # Create generators
        self.train_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            shuffle=True
        )

        self.val_generator = val_test_datagen.flow_from_directory(
            val_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            shuffle=False
        )

        self.test_generator = val_test_datagen.flow_from_directory(
            test_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            shuffle=False
        )

        print(f"Training samples: {self.train_generator.samples}")
        print(f"Validation samples: {self.val_generator.samples}")
        print(f"Test samples: {self.test_generator.samples}")

    def build_cnn_model(self):
        """Build custom CNN model"""
        self.model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(64, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(128, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(256, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Flatten(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])

        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        print("Custom CNN model built successfully")
        return self.model

    def build_transfer_learning_model(self, base_model='vgg16'):
        """Build transfer learning model"""
        if base_model.lower() == 'vgg16':
            base = VGG16(weights='imagenet', include_top=False, input_shape=(*self.img_size, 3))
        elif base_model.lower() == 'resnet50':
            base = ResNet50(weights='imagenet', include_top=False, input_shape=(*self.img_size, 3))
        else:
            raise ValueError("Supported models: 'vgg16', 'resnet50'")

        # Freeze base model layers
        base.trainable = False

        # Add custom classification head
        self.model = Sequential([
            base,
            tf.keras.layers.GlobalAveragePooling2D(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])

        self.model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        print(f"Transfer learning model ({base_model}) built successfully")
        return self.model

    def train(self, epochs=50, use_callbacks=True):
        """Train the model"""
        callbacks = []

        if use_callbacks:
            # Early stopping
            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )

            # Reduce learning rate
            reduce_lr = ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=5,
                min_lr=1e-7,
                verbose=1
            )

            # Model checkpoint
            checkpoint = ModelCheckpoint(
                'best_medical_model.h5',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )

            callbacks = [early_stop, reduce_lr, checkpoint]

        # Calculate steps
        steps_per_epoch = self.train_generator.samples // self.batch_size
        validation_steps = self.val_generator.samples // self.batch_size

        # Train model
        self.history = self.model.fit(
            self.train_generator,
            steps_per_epoch=steps_per_epoch,
            validation_data=self.val_generator,
            validation_steps=validation_steps,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )

        return self.history

    def evaluate(self):
        """Evaluate model on test data"""
        # Predictions
        test_steps = self.test_generator.samples // self.batch_size
        predictions = self.model.predict(self.test_generator, steps=test_steps)
        predicted_classes = (predictions > 0.5).astype(int)

        # True labels
        true_labels = self.test_generator.classes[:len(predicted_classes)]

        # Classification report
        report = classification_report(
            true_labels, predicted_classes,
            target_names=self.class_names
        )
        print("Classification Report:")
        print(report)

        # Confusion matrix
        cm = confusion_matrix(true_labels, predicted_classes)

        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names)
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()

        return report, cm

    def plot_training_history(self):
        """Plot training history"""
        if self.history is None:
            print("Model hasn't been trained yet")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # Accuracy plot
        ax1.plot(self.history.history['accuracy'], label='Training Accuracy')
        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()

        # Loss plot
        ax2.plot(self.history.history['loss'], label='Training Loss')
        ax2.plot(self.history.history['val_loss'], label='Validation Loss')
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()

        plt.tight_layout()
        plt.show()

    def predict_single_image(self, image_path):
        """Predict single image"""
        # Load and preprocess image
        img = Image.open(image_path).convert('RGB')
        img = img.resize(self.img_size)
        img_array = np.array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        # Make prediction
        prediction = self.model.predict(img_array)[0][0]
        predicted_class = self.class_names[1] if prediction > 0.5 else self.class_names[0]
        confidence = prediction if prediction > 0.5 else 1 - prediction

        # Display result
        plt.figure(figsize=(8, 6))
        plt.imshow(img)
        plt.title(f'Prediction: {predicted_class} (Confidence: {confidence:.2f})')
        plt.axis('off')
        plt.show()

        return predicted_class, confidence

    def gradcam_visualization(self, image_path, layer_name='block5_conv3'):
        """Generate GradCAM visualization for model interpretability"""
        try:
            # Load and preprocess image
            img = Image.open(image_path).convert('RGB')
            img = img.resize(self.img_size)
            img_array = np.array(img) / 255.0
            img_array = np.expand_dims(img_array, axis=0)

            # Create GradCAM model
            grad_model = tf.keras.models.Model(
                [self.model.inputs],
                [self.model.get_layer(layer_name).output, self.model.output]
            )

            # Get gradients
            with tf.GradientTape() as tape:
                conv_outputs, predictions = grad_model(img_array)
                loss = predictions[:, 0]

            # Calculate gradients
            grads = tape.gradient(loss, conv_outputs)
            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

            # Generate heatmap
            conv_outputs = conv_outputs[0]
            heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)
            heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
            heatmap = heatmap.numpy()

            # Resize heatmap
            heatmap = cv2.resize(heatmap, self.img_size)
            heatmap = np.uint8(255 * heatmap)
            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Overlay on original image
            img_array_uint8 = np.uint8(255 * img_array[0])
            overlayed = cv2.addWeighted(img_array_uint8, 0.6, heatmap, 0.4, 0)

            # Display
            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
            ax1.imshow(img)
            ax1.set_title('Original Image')
            ax1.axis('off')

            ax2.imshow(heatmap)
            ax2.set_title('GradCAM Heatmap')
            ax2.axis('off')

            ax3.imshow(overlayed)
            ax3.set_title('Overlay')
            ax3.axis('off')

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"GradCAM visualization failed: {e}")

def main():
    # Initialize classifier
    classifier = MedicalImageClassifier(img_size=(224, 224), batch_size=32)

    # Set data directories (adjust paths as needed)
    train_dir = 'data/chest_xray/train'
    val_dir = 'data/chest_xray/val'
    test_dir = 'data/chest_xray/test'

    # Create data generators
    classifier.create_data_generators(train_dir, val_dir, test_dir)

    # Build model (choose one)
    # classifier.build_cnn_model()  # Custom CNN
    classifier.build_transfer_learning_model('vgg16')  # Transfer learning

    # Train model
    history = classifier.train(epochs=30)

    # Plot training history
    classifier.plot_training_history()

    # Evaluate model
    classifier.evaluate()

    # Test single prediction
    # classifier.predict_single_image('path/to/test/image.jpg')

if __name__ == "__main__":
    main()